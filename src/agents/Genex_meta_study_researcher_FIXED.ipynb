{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1edd64e0",
   "metadata": {},
   "source": [
    "# Genex Meta-Study Researcher (Fixed)\n",
    "\n",
    "This notebook screens PDFs for relevance, extracts structured findings, and enables grounded Q&A with citations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56f338b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 0) Imports\n",
    "import os, re, json, math\n",
    "from typing import Any, Dict, List, Optional\n",
    "import uuid\n",
    "\n",
    "import fitz  # PyMuPDF\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from google.adk.agents import LlmAgent\n",
    "from google.adk.runners import Runner\n",
    "from google.adk.sessions import InMemorySessionService\n",
    "from google.adk.models.lite_llm import LiteLlm\n",
    "\n",
    "try:\n",
    "    from google.genai.types import Content, Part\n",
    "    _HAS_GENAI_TYPES = True\n",
    "except Exception:\n",
    "    Content, Part = None, None\n",
    "    _HAS_GENAI_TYPES = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7bc58f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1) Settings\n",
    "PAPERS_DIR = r\"C:/Users/T490/Downloads/Genetics-Dashboard/docs/papers/serine_deficiency_papers/\"\n",
    "MAX_PAGES_PER_PDF = 25\n",
    "MODEL = \"openai/gpt-4o-mini\"\n",
    "\n",
    "APP_NAME = \"genex_meta_study\"\n",
    "USER_ID = \"genex_user\"\n",
    "\n",
    "llm = LiteLlm(model=MODEL)\n",
    "session_service = InMemorySessionService()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ed259cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2) Schemas\n",
    "class PaperMetadata(BaseModel):\n",
    "    title: str\n",
    "    authors: List[str] = Field(default_factory=list)\n",
    "    year: Optional[int] = None\n",
    "    journal: Optional[str] = None\n",
    "    doi: Optional[str] = None\n",
    "\n",
    "class RelevanceDecision(BaseModel):\n",
    "    paper_id: str\n",
    "    title: str\n",
    "    is_relevant: bool = False\n",
    "    relevance_score: float = 0.0\n",
    "    reason: str = \"\"\n",
    "    matched_terms: List[str] = Field(default_factory=list)\n",
    "\n",
    "class ExtractedFinding(BaseModel):\n",
    "    name: str\n",
    "    category: str\n",
    "    polarity: str\n",
    "    snippet: str\n",
    "    section: str\n",
    "\n",
    "class PaperExtraction(BaseModel):\n",
    "    paper_id: str\n",
    "    title: str\n",
    "    authors: List[str] = Field(default_factory=list)\n",
    "    year: Optional[int] = None\n",
    "    journal: Optional[str] = None\n",
    "    doi: Optional[str] = None\n",
    "    condition: str = \"\"\n",
    "    relevance_score: float = 0.0\n",
    "    summary: str = \"\"\n",
    "    key_takeaways: List[str] = Field(default_factory=list)\n",
    "    findings: List[ExtractedFinding] = Field(default_factory=list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7be08ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3) Helpers (robust across ADK versions)\n",
    "def list_pdfs(folder: str) -> List[str]:\n",
    "    return sorted(\n",
    "        os.path.join(folder, f)\n",
    "        for f in os.listdir(folder)\n",
    "        if f.lower().endswith(\".pdf\")\n",
    "    )\n",
    "\n",
    "def pdf_to_text(path: str, max_pages: int = 20) -> str:\n",
    "    doc = fitz.open(path)\n",
    "    out = []\n",
    "    for i in range(min(len(doc), max_pages)):\n",
    "        out.append(doc.load_page(i).get_text(\"text\"))\n",
    "    doc.close()\n",
    "    return \"\\n\".join(out)\n",
    "\n",
    "def safe_json_extract(text: str) -> Optional[Dict[str, Any]]:\n",
    "    if not text or not isinstance(text, str):\n",
    "        return None\n",
    "    text = text.strip()\n",
    "    try:\n",
    "        obj = json.loads(text)\n",
    "        return obj if isinstance(obj, dict) else None\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    fence = re.search(r\"```json\\s*(\\{[\\s\\S]*?\\})\\s*```\", text, flags=re.IGNORECASE)\n",
    "    if fence:\n",
    "        try:\n",
    "            obj = json.loads(fence.group(1))\n",
    "            return obj if isinstance(obj, dict) else None\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    m = re.search(r\"(\\{[\\s\\S]*\\})\", text)\n",
    "    if m:\n",
    "        try:\n",
    "            obj = json.loads(m.group(1))\n",
    "            return obj if isinstance(obj, dict) else None\n",
    "        except Exception:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "def _make_message(text: str):\n",
    "    if _HAS_GENAI_TYPES and Content is not None and Part is not None:\n",
    "        return Content(parts=[Part(text=text)])\n",
    "    return text\n",
    "\n",
    "async def collect_events(async_gen) -> List[Any]:\n",
    "    events = []\n",
    "    async for e in async_gen:\n",
    "        events.append(e)\n",
    "    return events\n",
    "\n",
    "async def run_runner(runner: Runner, user_id: str, session_id: str, text: str) -> List[Any]:\n",
    "    res = runner.run_async(user_id=user_id, session_id=session_id, new_message=_make_message(text))\n",
    "    if hasattr(res, \"__aiter__\"):\n",
    "        return await collect_events(res)\n",
    "    out = await res\n",
    "    if isinstance(out, list):\n",
    "        return out\n",
    "    return [out]\n",
    "\n",
    "def _event_to_text(e: Any) -> str:\n",
    "    if e is None:\n",
    "        return \"\"\n",
    "    if isinstance(e, str):\n",
    "        return e\n",
    "    if isinstance(e, dict):\n",
    "        for k in (\"text\", \"output_text\"):\n",
    "            if isinstance(e.get(k), str):\n",
    "                return e[k]\n",
    "        content = e.get(\"content\")\n",
    "        if isinstance(content, dict):\n",
    "            parts = content.get(\"parts\") or []\n",
    "            texts = []\n",
    "            for p in parts:\n",
    "                if isinstance(p, dict) and isinstance(p.get(\"text\"), str):\n",
    "                    texts.append(p[\"text\"])\n",
    "            return \"\\n\".join(texts)\n",
    "    for attr in (\"text\", \"output_text\"):\n",
    "        if hasattr(e, attr):\n",
    "            v = getattr(e, attr)\n",
    "            if isinstance(v, str):\n",
    "                return v\n",
    "    if hasattr(e, \"content\"):\n",
    "        c = getattr(e, \"content\")\n",
    "        if hasattr(c, \"parts\"):\n",
    "            texts = []\n",
    "            for p in getattr(c, \"parts\") or []:\n",
    "                if hasattr(p, \"text\") and isinstance(getattr(p, \"text\"), str):\n",
    "                    texts.append(getattr(p, \"text\"))\n",
    "                elif isinstance(p, dict) and isinstance(p.get(\"text\"), str):\n",
    "                    texts.append(p[\"text\"])\n",
    "            if texts:\n",
    "                return \"\\n\".join(texts)\n",
    "        if isinstance(c, str):\n",
    "            return c\n",
    "    return \"\"\n",
    "\n",
    "def extract_last_text(events: List[Any]) -> str:\n",
    "    for e in reversed(events or []):\n",
    "        t = _event_to_text(e).strip()\n",
    "        if t:\n",
    "            return t\n",
    "    return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef764c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4) Agents\n",
    "\n",
    "METADATA_SYSTEM = '''\n",
    "You extract bibliographic metadata from the first pages of a biomedical paper.\n",
    "\n",
    "Return ONE valid JSON object ONLY with EXACTLY these keys:\n",
    "{\n",
    "  \"title\": \"...\",\n",
    "  \"authors\": [\"Last, First\", \"...\"],\n",
    "  \"year\": null,\n",
    "  \"journal\": null,\n",
    "  \"doi\": null\n",
    "}\n",
    "\n",
    "Rules:\n",
    "- If unsure, use null (or [] for authors).\n",
    "- Do not hallucinate.\n",
    "'''\n",
    "\n",
    "RELEVANCE_SYSTEM = '''\n",
    "You decide whether a paper is relevant to the CONDITION.\n",
    "\n",
    "Return ONE valid JSON object ONLY with EXACTLY these keys:\n",
    "{\n",
    "  \"paper_id\": \"...\",\n",
    "  \"title\": \"...\",\n",
    "  \"is_relevant\": false,\n",
    "  \"relevance_score\": 0.0,\n",
    "  \"reason\": \"...\",\n",
    "  \"matched_terms\": [\"...\", \"...\"]\n",
    "}\n",
    "\n",
    "Rules:\n",
    "- Use ONLY the provided text.\n",
    "- relevance_score is 0..1.\n",
    "- If uncertain, be conservative.\n",
    "'''\n",
    "\n",
    "EXTRACTION_SYSTEM = '''\n",
    "You extract structured findings from a paper for the CONDITION.\n",
    "\n",
    "Return ONE valid JSON object ONLY with EXACTLY these keys:\n",
    "{\n",
    "  \"paper_id\": \"...\",\n",
    "  \"title\": \"...\",\n",
    "  \"year\": null,\n",
    "  \"journal\": null,\n",
    "  \"condition\": \"...\",\n",
    "  \"relevance_score\": 0.0,\n",
    "  \"summary\": \"...\",\n",
    "  \"key_takeaways\": [\"...\", \"...\"],\n",
    "  \"findings\": [\n",
    "    {\"name\":\"...\", \"category\":\"definition|gene|symptom|treatment|outcome|population|other\",\n",
    "     \"polarity\":\"supports|refutes|mixed|unclear\",\n",
    "     \"snippet\":\"<=2 sentences\", \"section\":\"methods|results|discussion|abstract|unknown\"}\n",
    "  ]\n",
    "}\n",
    "\n",
    "Rules:\n",
    "- Use ONLY the provided text.\n",
    "- Do NOT invent details.\n",
    "'''\n",
    "\n",
    "QA_SYSTEM = '''\n",
    "You are a biomedical literature Q&A agent.\n",
    "\n",
    "You will be given:\n",
    "- a user question\n",
    "- evidence items (snippets) extracted from papers, each with citation fields\n",
    "\n",
    "Your job:\n",
    "- Answer ONLY using the evidence items.\n",
    "- If the evidence does not contain an answer, say: \"Not found in the provided papers.\"\n",
    "- Use inline citations like [1], [2] corresponding to evidence item IDs.\n",
    "\n",
    "Output Markdown:\n",
    "1) Answer\n",
    "2) Evidence (bullets with snippet + citation number)\n",
    "3) References (numbered: title, authors, journal, year; include DOI if present)\n",
    "Do not invent papers or details.\n",
    "'''\n",
    "\n",
    "metadata_agent = LlmAgent(name=\"PaperMetadataExtractor\", model=llm, instruction=METADATA_SYSTEM)\n",
    "relevance_agent = LlmAgent(name=\"PaperRelevanceScreener\", model=llm, instruction=RELEVANCE_SYSTEM)\n",
    "extraction_agent = LlmAgent(name=\"PaperExtractor\", model=llm, instruction=EXTRACTION_SYSTEM)\n",
    "qa_agent = LlmAgent(name=\"PaperQAAgent\", model=llm, instruction=QA_SYSTEM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84cb28be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5) Pipeline\n",
    "\n",
    "def infer_meta_from_text(paper_id: str, path: str, raw_text: str) -> Dict[str, Any]:\n",
    "    title = os.path.splitext(os.path.basename(path))[0]\n",
    "    year = None\n",
    "    journal = None\n",
    "    m = re.search(r\"(19\\d{2}|20\\d{2})\", raw_text[:4000])\n",
    "    if m:\n",
    "        try:\n",
    "            year = int(m.group(1))\n",
    "        except Exception:\n",
    "            year = None\n",
    "    return {\"paper_id\": paper_id, \"path\": path, \"title\": title, \"year\": year, \"journal\": journal, \"authors\": [], \"doi\": None}\n",
    "\n",
    "async def extract_metadata(paper_id: str, path: str) -> Dict[str, Any]:\n",
    "    meta_text = pdf_to_text(path, max_pages=2)[:60000]\n",
    "    meta = infer_meta_from_text(paper_id, path, meta_text)\n",
    "\n",
    "    session_id = f\"meta-{paper_id}\"\n",
    "    await session_service.create_session(app_name=APP_NAME, user_id=USER_ID, session_id=session_id)\n",
    "    runner = Runner(app_name=APP_NAME, agent=metadata_agent, session_service=session_service)\n",
    "\n",
    "    msg = f\"PAPER_ID: {paper_id}\\nFILENAME: {os.path.basename(path)}\\n\\nTEXT (first pages):\\n{meta_text}\"\n",
    "    events = await run_runner(runner, user_id=USER_ID, session_id=session_id, text=msg)\n",
    "    meta_json = safe_json_extract(extract_last_text(events))\n",
    "\n",
    "    if isinstance(meta_json, dict):\n",
    "        meta[\"title\"] = meta_json.get(\"title\") or meta[\"title\"]\n",
    "        meta[\"journal\"] = meta_json.get(\"journal\") or meta[\"journal\"]\n",
    "        meta[\"year\"] = meta_json.get(\"year\") or meta[\"year\"]\n",
    "        meta[\"doi\"] = meta_json.get(\"doi\") or meta.get(\"doi\")\n",
    "        meta[\"authors\"] = meta_json.get(\"authors\") or meta.get(\"authors\", [])\n",
    "    return meta\n",
    "\n",
    "async def screen_relevance(condition: str, paper_id: str, title: str, text: str) -> RelevanceDecision:\n",
    "    session_id = f\"rel-{paper_id}\"\n",
    "    await session_service.create_session(app_name=APP_NAME, user_id=USER_ID, session_id=session_id)\n",
    "    runner = Runner(app_name=APP_NAME, agent=relevance_agent, session_service=session_service)\n",
    "\n",
    "    prompt = f\"CONDITION: {condition}\\nPAPER_ID: {paper_id}\\nTITLE: {title}\\n\\nTEXT:\\n{text[:120000]}\"\n",
    "    events = await run_runner(runner, user_id=USER_ID, session_id=session_id, text=prompt)\n",
    "    obj = safe_json_extract(extract_last_text(events)) or {}\n",
    "    obj.setdefault(\"paper_id\", paper_id)\n",
    "    obj.setdefault(\"title\", title)\n",
    "    return RelevanceDecision(**obj)\n",
    "\n",
    "async def extract_paper(condition: str, paper_id: str, meta: Dict[str, Any], text: str, relevance_score: float) -> PaperExtraction:\n",
    "    session_id = f\"ext-{paper_id}\"\n",
    "    await session_service.create_session(app_name=APP_NAME, user_id=USER_ID, session_id=session_id)\n",
    "    runner = Runner(app_name=APP_NAME, agent=extraction_agent, session_service=session_service)\n",
    "\n",
    "    prompt = (\n",
    "        f\"CONDITION: {condition}\\nPAPER_ID: {paper_id}\\n\"\n",
    "        f\"TITLE: {meta.get('title')}\\nYEAR: {meta.get('year')}\\nJOURNAL: {meta.get('journal')}\\n\"\n",
    "        f\"AUTHORS: {', '.join(meta.get('authors') or [])}\\nDOI: {meta.get('doi')}\\n\\n\"\n",
    "        f\"TEXT:\\n{text[:140000]}\"\n",
    "    )\n",
    "    events = await run_runner(runner, user_id=USER_ID, session_id=session_id, text=prompt)\n",
    "    obj = safe_json_extract(extract_last_text(events)) or {}\n",
    "\n",
    "    obj[\"paper_id\"] = paper_id\n",
    "    obj[\"title\"] = meta.get(\"title\") or obj.get(\"title\") or \"\"\n",
    "    obj[\"authors\"] = meta.get(\"authors\") or obj.get(\"authors\") or []\n",
    "    obj[\"year\"] = meta.get(\"year\") or obj.get(\"year\")\n",
    "    obj[\"journal\"] = meta.get(\"journal\") or obj.get(\"journal\")\n",
    "    obj[\"doi\"] = meta.get(\"doi\") or obj.get(\"doi\")\n",
    "    obj[\"condition\"] = condition\n",
    "    obj[\"relevance_score\"] = float(relevance_score or obj.get(\"relevance_score\") or 0.0)\n",
    "\n",
    "    if not isinstance(obj.get(\"findings\"), list):\n",
    "        obj[\"findings\"] = []\n",
    "    return PaperExtraction(**obj)\n",
    "\n",
    "async def run_condition_folder(condition: str, folder: str) -> Dict[str, Any]:\n",
    "    pdfs = list_pdfs(folder)\n",
    "    papers = []\n",
    "    papers_screened = 0\n",
    "    papers_relevant = 0\n",
    "\n",
    "    for idx, path in enumerate(pdfs, start=1):\n",
    "        paper_id = f\"paper_{idx:03d}\"\n",
    "        raw_text = pdf_to_text(path, max_pages=MAX_PAGES_PER_PDF)\n",
    "        papers_screened += 1\n",
    "\n",
    "        meta = await extract_metadata(paper_id, path)\n",
    "        rel = await screen_relevance(condition, paper_id, meta.get(\"title\") or paper_id, raw_text)\n",
    "\n",
    "        if (not rel.is_relevant) or (rel.relevance_score < 0.20):\n",
    "            continue\n",
    "\n",
    "        papers_relevant += 1\n",
    "        extraction = await extract_paper(condition, paper_id, meta, raw_text, rel.relevance_score)\n",
    "        papers.append(extraction.model_dump())\n",
    "\n",
    "    md = [f\"# Meta-study report: {condition}\", \"\", f\"- Papers screened: **{papers_screened}**\", f\"- Papers relevant: **{papers_relevant}**\", \"\"]\n",
    "    for p in papers:\n",
    "        authors = \", \".join(p.get(\"authors\") or []) or \"UNKNOWN\"\n",
    "        j = p.get(\"journal\") or \"UNKNOWN JOURNAL\"\n",
    "        y = p.get(\"year\") or \"n.d.\"\n",
    "        md += [f\"## {p.get('title','(untitled)')}\", f\"**Authors:** {authors}  \", f\"**Journal/Year:** {j} ({y})  \"]\n",
    "        if p.get(\"doi\"):\n",
    "            md.append(f\"**DOI:** {p.get('doi')}  \")\n",
    "        md.append(\"\")\n",
    "        if p.get(\"summary\"):\n",
    "            md += [p[\"summary\"], \"\"]\n",
    "        if p.get(\"findings\"):\n",
    "            md.append(\"**Findings (top 10):**\")\n",
    "            for f in p[\"findings\"][:10]:\n",
    "                md.append(f\"- **{f.get('category')}**: {f.get('name')} ({f.get('polarity')}) — {f.get('snippet')}\")\n",
    "            md.append(\"\")\n",
    "\n",
    "    return {\"condition\": condition, \"papers_screened\": papers_screened, \"papers_relevant\": papers_relevant, \"papers\": papers, \"report_markdown\": \"\\n\".join(md)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a37831ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 6) Q&A\n",
    "\n",
    "def build_evidence_index(papers: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    rows = []\n",
    "    for p in papers:\n",
    "        for f in (p.get(\"findings\") or []):\n",
    "            rows.append({\n",
    "                \"paper_id\": p.get(\"paper_id\"),\n",
    "                \"title\": p.get(\"title\"),\n",
    "                \"authors\": p.get(\"authors\") or [],\n",
    "                \"year\": p.get(\"year\"),\n",
    "                \"journal\": p.get(\"journal\"),\n",
    "                \"doi\": p.get(\"doi\"),\n",
    "                \"condition\": p.get(\"condition\"),\n",
    "                \"relevance_score\": p.get(\"relevance_score\", 0.0),\n",
    "                \"name\": f.get(\"name\"),\n",
    "                \"category\": f.get(\"category\"),\n",
    "                \"polarity\": f.get(\"polarity\"),\n",
    "                \"section\": f.get(\"section\"),\n",
    "                \"snippet\": f.get(\"snippet\"),\n",
    "            })\n",
    "    return rows\n",
    "\n",
    "def _tokenize(s: str) -> List[str]:\n",
    "    return re.findall(r\"[a-z0-9]+\", (s or \"\").lower())\n",
    "\n",
    "def retrieve_evidence(question: str, evidence_rows: List[Dict[str, Any]], top_k: int = 14) -> List[Dict[str, Any]]:\n",
    "    qtok = set(_tokenize(question))\n",
    "    if not qtok:\n",
    "        return evidence_rows[:top_k]\n",
    "    scored = []\n",
    "    for r in evidence_rows:\n",
    "        blob = \" \".join([str(r.get(\"name\",\"\")), str(r.get(\"category\",\"\")), str(r.get(\"snippet\",\"\")), str(r.get(\"title\",\"\")), str(r.get(\"journal\",\"\"))]).lower()\n",
    "        btok = set(_tokenize(blob))\n",
    "        overlap = len(qtok & btok)\n",
    "        boost = 0.0\n",
    "        if r.get(\"polarity\") == \"supports\":\n",
    "            boost += 0.25\n",
    "        boost += 0.15 * float(r.get(\"relevance_score\") or 0.0)\n",
    "        score = overlap + boost\n",
    "        if overlap > 0:\n",
    "            scored.append((score, r))\n",
    "    scored.sort(key=lambda x: x[0], reverse=True)\n",
    "    return [r for _, r in scored[:top_k]]\n",
    "\n",
    "async def ask_papers(question: str, result: Dict[str, Any], top_k: int = 14) -> str:\n",
    "    papers = result.get(\"papers\") or []\n",
    "    evidence_rows = build_evidence_index(papers)\n",
    "    top = retrieve_evidence(question, evidence_rows, top_k=top_k)\n",
    "\n",
    "    lines = []\n",
    "    for i, r in enumerate(top, start=1):\n",
    "        authors = \", \".join(r.get(\"authors\") or [])\n",
    "        lines.append(\n",
    "            f\"EVIDENCE_ITEM [{i}]\\n\"\n",
    "            f\"title: {r.get('title')}\\n\"\n",
    "            f\"authors: {authors if authors else 'UNKNOWN'}\\n\"\n",
    "            f\"journal: {r.get('journal')}\\n\"\n",
    "            f\"year: {r.get('year')}\\n\"\n",
    "            f\"doi: {r.get('doi')}\\n\"\n",
    "            f\"snippet: {r.get('snippet')}\\n\"\n",
    "        )\n",
    "    qa_context = \"\\n\".join(lines) if lines else \"NO EVIDENCE ITEMS RETRIEVED.\"\n",
    "\n",
    "    session_id = f\"qa-{uuid.uuid4().hex[:8]}\"\n",
    "    await session_service.create_session(app_name=APP_NAME, user_id=USER_ID, session_id=session_id)\n",
    "    runner = Runner(app_name=APP_NAME, agent=qa_agent, session_service=session_service)\n",
    "\n",
    "    prompt = f\"USER QUESTION:\\n{question}\\n\\n{qa_context}\"\n",
    "    events = await run_runner(runner, user_id=USER_ID, session_id=session_id, text=prompt)\n",
    "    return extract_last_text(events)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d6a31ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 7) Example run (uncomment)\n",
    "\n",
    "# If your notebook doesn't support top-level `await`, run:\n",
    "# import nest_asyncio; nest_asyncio.apply()\n",
    "\n",
    "# result = await run_condition_folder(\"L-serine deficiency disorder\", PAPERS_DIR)\n",
    "# print(result[\"papers_screened\"], result[\"papers_relevant\"])\n",
    "# print(result[\"report_markdown\"][:1200])\n",
    "# print(await ask_papers(\"Are there different types of serine deficiency?\", result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ab05b2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio; nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba4cd816",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await run_condition_folder(\"L-serine deficiency disorder\", PAPERS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "72d1936e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 10\n"
     ]
    }
   ],
   "source": [
    "print(result[\"papers_screened\"], result[\"papers_relevant\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a84a2d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Meta-study report: L-serine deficiency disorder\n",
      "\n",
      "- Papers screened: **11**\n",
      "- Papers relevant: **10**\n",
      "\n",
      "## L-Serine in disease and development\n",
      "**Authors:** De Koning, Tom J., Snell, Keith, Duran, Marinus, Berger, Ruud, Poll-The, Bwee-Tien, Surtees, Robert  \n",
      "**Journal/Year:** Biochemical Journal (2003)  \n",
      "\n",
      "This review discusses the role of L-serine in metabolism and its importance in the development and function of the central nervous system, emphasizing its conditional essentiality during specific developmental stages and in various diseases, including L-serine biosynthesis disorders.\n",
      "\n",
      "**Findings (top 10):**\n",
      "- **definition**: L-serine's role in the synthesis of neuromodulators (supports) — The formation of glycine from L-serine is an important reaction; not only does it result in the transfer of a one-carbon group to folates, but also the glycine itself has important functions, particularly in the central nervous system.\n",
      "- **outcome**: Impact of L-serine deficiency on the central nervous system (supports) — Serine and glycine deficiencies underline the importance of the serine biosynthesis pathway for brain development and function, with neurological manifestations noted in related \n"
     ]
    }
   ],
   "source": [
    "print(result[\"report_markdown\"][:1200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93d2eae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) Yes, there are different types of serine deficiency, classified under serine deficiency disorders which include a spectrum of disease from lethal prenatal-onset conditions to those with infantile, juvenile, or adult onset phenotypes.\n",
      "\n",
      "2) Evidence:\n",
      "   - \"Serine deficiency disorders include a spectrum of disease ranging from lethal prenatal-onset Neu-Laxova syndrome to serine deficiency with infantile, juvenile, or adult onset.\" [4]\n",
      "   - \"The patients presented with ichthyosis and juvenile-onset neuropathy, demonstrating a mild phenotype of serine deficiency disorder.\" [11]\n",
      "\n",
      "3) References:\n",
      "   1. \"Serine Deficiency Disorders,\" van der Crabben, Saskia N, de Koning, Tom J, GeneReviews, 2023. DOI: None.\n",
      "   2. \"Juvenile-onset PSAT1-related neuropathy: A milder phenotype of serine deﬁciency disorder,\" Shen, Yu, et al., Frontiers in Genetics, 2022. DOI: 10.3389/fgene.2022.949038.\n"
     ]
    }
   ],
   "source": [
    "print(await ask_papers(\"Are there different types of serine deficiency?\", result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bf14d166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) The genes linked to serine deficiency disorder include PSAT1 and PSPH. Mutations in these genes have been associated with various phenotypes of serine deficiency disorders, such as juvenile-onset neuropathy and Neu-Laxova syndrome.\n",
      "\n",
      "2) Evidence:\n",
      "   - \"Both patients presented with ichthyosis and juvenile-onset neuropathy attributed to the homozygous mutation c.43G > C (p.A15P) in the PSAT1 gene\" [1].\n",
      "   - \"We identified six families with PSAT1 mutations fully segregating with the disease, linking this gene to NLS\" [6].\n",
      "   - \"A homozygous frameshift mutation in PSPH was discovered in another family, contributing to the pathology of NLS\" [7].\n",
      "   - \"The study provides evidence that NLS is genetically heterogeneous, involving multiple mutations in genes encoding L-serine biosynthesis enzymes\" [9].\n",
      "\n",
      "3) References:\n",
      "   1. Juvenile-onset PSAT1-related neuropathy: A milder phenotype of serine deficiency disorder. Shen, Yu et al. Frontiers in Genetics, 2022. DOI: 10.3389/fgene.2022.949038.\n",
      "   2. Neu-Laxova Syndrome Is a Heterogeneous Metabolic Disorder Caused by Defects in Enzymes of the L-Serine Biosynthesis Pathway. Acuña-Hidalgo, Rocio et al. The American Journal of Human Genetics, 2014. DOI: 10.1016/j.ajhg.2014.07.012.\n"
     ]
    }
   ],
   "source": [
    "print(await ask_papers(\"what are the genes linked to serine deficiency disorder?\", result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa7406c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GeneX (venv)",
   "language": "python",
   "name": "genex-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
